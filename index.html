<!--
=========================================================
* Argon Design System - v1.2.2
=========================================================

* Product Page: https://www.creative-tim.com/product/argon-design-system
* Copyright 2020 Creative Tim (https://www.creative-tim.com)

Coded by www.creative-tim.com

=========================================================

* The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. -->
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <title>
    Cassandra Event
  </title>
  <!--     Fonts and icons     -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,600,700" rel="stylesheet">
  <link href="https://use.fontawesome.com/releases/v5.0.6/css/all.css" rel="stylesheet">
  <!-- Nucleo Icons -->
  <link href="./assets/css/nucleo-icons.css" rel="stylesheet" />
  <link href="./assets/css/nucleo-svg.css" rel="stylesheet" />
  <!-- Font Awesome Icons -->
  <link href="./assets/css/font-awesome.css" rel="stylesheet" />
  <link href="./assets/css/nucleo-svg.css" rel="stylesheet" />
  <!-- CSS Files -->
  <link href="./assets/css/argon-design-system.css?v=1.2.2" rel="stylesheet" />
</head>

<body class="index-page">
  <!-- Navbar -->
  <nav id="navbar-main" class="navbar navbar-main navbar-expand-lg bg-white navbar-light position-sticky top-0 shadow py-2">
    <div class="container">
      <a class="navbar-brand mr-lg-5" href="./index.html">
        <!-- <img src="./assets/img/brand/blue.png"> -->
        <img src="./statics/black.png" style="width:60px;height: 60px;">

      </a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar_global" aria-controls="navbar_global" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="navbar-collapse collapse" id="navbar_global">
        <div class="navbar-collapse-header">
          <div class="row">
            <div class="col-6 collapse-brand">
              <a href="./index.html">
                <img src="./statics/black.png">
                <!-- inja ye kari kardam nemidonam -->
              </a>
            </div>
            <div class="col-6 collapse-close">
              <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar_global" aria-controls="navbar_global" aria-expanded="false" aria-label="Toggle navigation">
                <span></span>
                <span></span>
              </button>
            </div>
          </div>
        </div>
        <ul class="navbar-nav navbar-nav-hover align-items-lg-center">
          <li class="nav-item dropdown">
            <a href="#day_1" class="nav-link" data-toggle="dropdown" role="button">
              <i class="ni ni-ui-04 d-lg-none"></i>
              <span class="nav-link-inner--text">Day one</span>
            </a>
          </li>
          <li class="nav-item dropdown">
            <a href="#day_2" class="nav-link" data-toggle="dropdown" role="button">
              <i class="ni ni-collection d-lg-none"></i>
              <span class="nav-link-inner--text">Day two</span>
            </a>
          </li>
        </ul>
        <ul class="navbar-nav align-items-lg-center ml-lg-auto">
          <li class="nav-item d-none d-lg-block">
            <a href="https://evand.com/events/cass-ai-day" target="_blank" class="btn btn-primary btn-icon">
              <span class="nav-link-inner--text">Registration</span>
            </a>
          </li>
        </ul>
      </div>
    </div>
  </nav>
  <!-- End Navbar -->
  <div class="wrapper" style="background-image: url('./assets/img/ill/1.svg');">
    <div class="section section-hero section-shaped">
      <div class="shape shape-style-1 shape-primary">
        <span class="span-150"></span>
        <span class="span-50"></span>
        <span class="span-50"></span>
        <span class="span-75"></span>
        <span class="span-100"></span>
        <span class="span-75"></span>
        <span class="span-50"></span>
        <span class="span-100"></span>
        <span class="span-50"></span>
        <span class="span-100"></span>
      </div>
      <div class="page-header">
        <div class="container shape-container d-flex align-items-center py-lg">
          <div class="col px-0">
            <div class="row align-items-center justify-content-center">
              <div class="col-lg-6 text-center">
                <img src="./statics/white.png" style="width: 300px;" class="img-fluid">
                <p class="lead text-white">CASSANDRA ARTIFICIAL INTELLIGENCE EVENT</p>
                <p class="lead text-white">Cassandra Artificial Intelligence event aims to present certain aspects of artificial intelligence. The event is hosted by Isfahan University students and will take place over two days on sunday and monday May 29 and May 30, 2022 at 16:00 GMT+4:30.</p>
                <p class="lead text-white">This event would be located at the University of Isfahan and meanwhile we will have a live stream for those who cannot come to the University of Isfahan. For the live stream, a website will be available shortly for registration. The language of the presentations would be Persian, but for additional information, comments in English would be written in individual repositories.</p>

                <div class="btn-wrapper mt-5">
                  <a href="https://github.com/cass-ai" class="btn btn-lg btn-github btn-icon mb-3 mb-sm-0" target="_blank">
                    <span class="btn-inner--icon"><i class="fa fa-github"></i></span>
                    <span class="btn-inner--text"><span class="text-warning">Star us</span> on Github</span>
                  </a>
                </div>
                <div class="mt-5">
                  <small class="font-weight-bold mb-0 mr-2 text-white">proudly coded by lots of coffee</small>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div class="separator separator-bottom separator-skew zindex-100">
        <svg x="0" y="0" viewBox="0 0 2560 100" preserveAspectRatio="none" version="1.1" xmlns="http://www.w3.org/2000/svg">
          <polygon class="fill-white" points="2560 0 2560 100 0 100"></polygon>
        </svg>
      </div>
    </div>
    <div class="section section-components pb-0" id="section-components">
      <div class="container">
        <div class="row justify-content-center">
          <div class="col-lg-12">
            <!-- Basic elements -->
            <h2 class="mb-5">
              <div class="row py-3 align-items-center">
                <div>
                  <h1 class="display-1 mb-0"  id="day_1">Instructor</h1>
                </div>
              </div>
            </h2>
          </div>
        </div>
      </div>
    </div>
<!-- Start of days section -->
    <div class="section" >
      <div class="container py-md">
        <div class="row justify-content-between align-items-center">
          <div class="col-lg-6 mb-lg-auto">
            <div class="rounded overflow-hidden transform-perspective-left">
              <img src="./statics/Dr_Baradaran.jpg" alt="Raised circle image" class="img-fluid rounded-circle " style="width: 150px;">
            </div>
          </div>
          <div class="col-lg-5 mb-5 mb-lg-0">
            <h1 class="font-weight-light">Dr. Hamidreza Baradaran</h1>
            <p class="lead mt-4">Natural Language Processing</p>
            <a href="https://engold.ui.ac.ir/~adibi/" class="btn btn-white mt-4">MORE</a>
          </div>
        </div>
      </div>
      </div>
      
<!-- End of days section -->
<div class="section section-components pb-0" id="section-components">
  <div class="container">
    <div class="row justify-content-center">
      <div class="col-lg-12">
        <!-- Basic elements -->
        <h2 class="mb-5">
          <div class="row py-3 align-items-center">
            <div>
              <h1 class="display-1 mb-0" id="day_2">Teaching Assistant</h1>
            </div>
          </div>
        </h2>
      </div>
    </div>
  </div>
</div>

<!-- Start of TA -->
<div class="section">
  <div class="container py-md">
    <div class="row justify-content-between align-items-center">
      <div class="col-lg-6 mb-lg-auto">
        <div class="rounded overflow-hidden transform-perspective-left">
          <img src="./statics/mehdi.PNG" alt="Raised circle image" class="img-fluid rounded-circle " style="width: 150px;">
        </div>
      </div>
      <div class="col-lg-5 mb-5 mb-lg-0">
        <h1 class="font-weight-light">Mehdi Darooni</h1>
        <p class="lead mt-4">An Introduction to geometric deep learning</p>
        <a href="https://github.com/CASS-AI/Day-2/tree/main/An-Introduction-to-geometric-deep-learning" class="btn btn-white mt-4">MORE</a>
      </div>
    </div>
  </div>
  </div>


<div class="section section-components pb-0" id="section-components">
  <div class="container">
    <div class="row justify-content-center">
      <div class="col-lg-12">
        <!-- Basic elements -->
        <h2 class="mb-5">
          <div class="row py-3 align-items-center">
            <div>
              <h1 class="display-1 mb-0" id="day_3">Schedule</h1>
            </div>
          </div>
        </h2>
      </div>
    </div>
  </div>
</div>



<!--
  stanford starts
-->
<!-- Schedule -->
<!-- Note the margin-top:-20px and the <br> serve to make the #schedule hyperlink display correctly (with the h2 header visible) -->
<div class="container sec" id="schedule" style="margin-top:-20px">
  <br>
  <h2>Schedule</h2>
  <p>
    Updated lecture <b>slides</b> will be posted here shortly before each lecture.
  </p>
  <p>
    Lecture <b>notes</b> will be uploaded a few days after most lectures. The notes (which cover approximately the first half of the course content) give supplementary detail beyond the lectures.
  </p>
  <table class="table">
    <colgroup>
      <col style="width:10%">
      <col style="width:20%">
      <col style="width:40%">
      <col style="width:10%">
      <col style="width:10%">
    </colgroup>
    <thead>
    <tr class="active">
      <th>Date</th>
      <th>Description</th>
      <th>Course Materials</th>
      <th>Events</th>
      <th>Deadlines</th>
    </tr>
    </thead>
    <tbody>
    <tr>
      <td>Tue Jan 4</td>
      <td>Word Vectors
        <br>
        [<a href="slides/cs224n-2022-lecture01-wordvecs1.pdf">slides</a>]
        <!--[<a href="https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=b2acdfb7-8038-49fb-941e-ab25012bd9ec">video</a>]-->
        [<a href="readings/cs224n-2019-notes01-wordvecs1.pdf">notes</a>]
        <br><br>
        Gensim word vectors example:
        <br>
        [<a href="materials/Gensim.zip">code</a>]
        [<a href="materials/Gensim%20word%20vector%20visualization.html">preview</a>]
      </td>
      <td>
        Suggested Readings:
        <ol>
          <li><a href="http://arxiv.org/pdf/1301.3781.pdf">Efficient Estimation of Word Representations in Vector Space</a> (original word2vec paper)</li>
          <li><a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Distributed Representations of Words and Phrases and their Compositionality</a> (negative sampling paper)</li>
        </ol>
      </td>
      <td>
        Assignment 1 <b><font color="green">out</font></b>
        <br>
        [<a href="assignments/a1.zip">code</a>]
        <br>
        [<a href="assignments/a1_preview/exploring_word_vectors.html">preview</a>]
      </td>
      <td></td>
    </tr>
  
    <tr>
      <td>Thu Jan 6</td>
      <td>Word Vectors 2 and Word Window Classification
        <br>
        [<a href="slides/cs224n-2022-lecture02-wordvecs2.pdf">slides</a>]
        <!--[<a href="">video</a>]-->
        [<a href="readings/cs224n-2019-notes02-wordvecs2.pdf">notes</a>]
      </td>
      <td>
        Suggested Readings:
        <ol>
          <li><a href="http://nlp.stanford.edu/pubs/glove.pdf">GloVe: Global Vectors for Word Representation</a> (original GloVe paper)</li>
          <li><a href="http://www.aclweb.org/anthology/Q15-1016">Improving Distributional Similarity with Lessons Learned from Word Embeddings</a></li>
          <li><a href="http://www.aclweb.org/anthology/D15-1036">Evaluation methods for unsupervised word embeddings</a></li>
        </ol>
        Additional Readings:
        <ol>
          <li><a href="http://aclweb.org/anthology/Q16-1028">A Latent Variable Model Approach to PMI-based Word Embeddings</a></li>
          <li><a href="https://transacl.org/ojs/index.php/tacl/article/viewFile/1346/320">Linear Algebraic Structure of Word Senses, with Applications to Polysemy</a></li>
          <li><a href="https://papers.nips.cc/paper/7368-on-the-dimensionality-of-word-embedding.pdf">On the Dimensionality of Word Embedding</a></li>
        </ol>
      </td>
      <td></td>
      <td></td>
    </tr>
  
    <tr class="warning">
      <td>Fri Jan 7</td>
      <td>Python Review Session
        <br>
        [<a href="readings/cs224n-python-review.pdf">slides</a>]
        [<a href="readings/python_tutorial.ipynb">notebook</a>]
      </td>
      <td>
        <i class="fa fa-clock-o"></i> 1:30pm - 2:30pm<br>Remote (link on Canvas)
      </td>
      <td></td>
      <td></td>
    </tr>
  
    <tr>
      <td>Tue Jan 11</td>
      <td>Backprop and Neural Networks
        <br>
        [<a href="slides/cs224n-2022-lecture03-neuralnets.pdf">slides</a>]
        [<a href="readings/cs224n-2019-notes03-neuralnets.pdf">notes</a>]
      </td>
      <td>
        Suggested Readings:
        <ol>
          <li><a href="readings/gradient-notes.pdf">matrix calculus notes</a></li>
          <li><a href="readings/review-differential-calculus.pdf">Review of differential calculus</a></li>
          <li><a href="http://cs231n.github.io/neural-networks-1/">CS231n notes on network architectures</a></li>
          <li><a href="http://cs231n.github.io/optimization-2/">CS231n notes on backprop</a></li>
          <li><a href="http://cs231n.stanford.edu/handouts/derivatives.pdf">Derivatives, Backpropagation, and Vectorization</a></li>
          <li><a href="http://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf">Learning Representations by Backpropagating Errors</a> (seminal Rumelhart et al. backpropagation paper)</li>
        </ol>
        Additional Readings:
        <ol>
          <li><a href="https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b">Yes you should understand backprop</a></li>
          <li><a href="http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf">Natural Language Processing (Almost) from Scratch</a></li>
        </ol>
      </td>
      <td>
        Assignment 2 <b><font color="green">out</font></b>
        <br>
        [<a href="assignments/a2.zip">code</a>]<br>
        [<a href="assignments/a2.pdf">handout</a>]<br>
        [<a href="assignments/a2_latex_template.zip">latex template</a>]
      </td>
      <td>Assignment 1 <b><font color="red">due</font></b></td>
    </tr>
  
    <tr>
      <td>Thu Jan 13</td>
      <td>Dependency Parsing
        <br>
        [<a href="slides/cs224n-2022-lecture04-dep-parsing.pdf">slides</a>]
        [<a href="readings/cs224n-2019-notes04-dependencyparsing.pdf">notes</a>]
        <br>
        [<a href="slides/cs224n-2021-lecture04-dep-parsing-annotated.pdf">slides (annotated)</a>]
      </td>
      <td>
        Suggested Readings:
        <ol>
          <li><a href="https://www.aclweb.org/anthology/W/W04/W04-0308.pdf">Incrementality in Deterministic Dependency Parsing</a></li>
          <li><a href="https://www.emnlp2014.org/papers/pdf/EMNLP2014082.pdf">A Fast and Accurate Dependency Parser using Neural Networks</a></li>
          <li><a href="http://www.morganclaypool.com/doi/abs/10.2200/S00169ED1V01Y200901HLT002">Dependency Parsing</a></li>
          <li><a href="https://arxiv.org/pdf/1603.06042.pdf">Globally Normalized Transition-Based Neural Networks</a></li>
          <li><a href="http://nlp.stanford.edu/~manning/papers/USD_LREC14_UD_revision.pdf">Universal Stanford Dependencies: A cross-linguistic typology</li>
          <li><a href="http://universaldependencies.org/">Universal Dependencies website</a></li>
          <li><a href="https://web.stanford.edu/~jurafsky/slp3/14.pdf">Jurafsky & Martin Chapter 14</a></li>
        </ol>
      </td>
      <td></td>
      <td></td>
    </tr>
  
    <tr class="warning">
      <td>Fri Jan 14</td>
      <td>PyTorch Tutorial Session
        <br>[<a href="https://colab.research.google.com/drive/13HGy3-uIIy1KD_WFhG4nVrxJC-3nUUkP?usp=sharing">colab notebook</a>]
      </td>
      <td>
        <i class="fa fa-clock-o"></i> 1:30pm - 2:30pm<br>
        Remote (link on Canvas)
      </td>
      <td></td>
      <td></td>
    </tr>
  
    <tr>
      <td>Tue Jan 18</td>
      <td>Recurrent Neural Networks and Language Models
        <br>
        [<a href="slides/cs224n-2022-lecture05-rnnlm.pdf">slides</a>]
         [<a href="readings/cs224n-2019-notes05-LM_RNN.pdf">notes (lectures 5 and 6)</a>]
      </td>
      <td>
        Suggested Readings:
        <ol>
          <li><a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf">N-gram Language Models</a> (textbook chapter)</li>
          <li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> (blog post overview)</li>
          <!-- <li><a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">Recurrent Neural Networks Tutorial</a> (practical guide)</li> -->
          <li><a href="http://www.deeplearningbook.org/contents/rnn.html">Sequence Modeling: Recurrent and Recursive Neural Nets</a> (Sections 10.1 and 10.2)</li>
         <li><a href="http://norvig.com/chomsky.html">On Chomsky and the Two Cultures of Statistical Learning</a>
        </ol>
      </td>
      <td>Assignment 3 <b><font color="green">out</font></b>
          <br>
          [<a href="assignments/a3.zip">code</a>]<br>
          [<a href="assignments/a3_handout.pdf">handout</a>]<br>
          [<a href="assignments/a3_latex.zip">latex template</a>]<br>
      </td>
      <td>Assignment 2 <b><font color="red">due</font></b></td>
    </tr>
  
    <tr>
      <td>Thu Jan 20</td>
      <td>Vanishing Gradients, Fancy RNNs, Seq2Seq
        <br>
        [<a href="slides/cs224n-2022-lecture06-fancy-rnn.pdf">slides</a>]
        [<a href="readings/cs224n-2019-notes05-LM_RNN.pdf">notes (lectures 5 and 6)</a>]
      </td>
      <td>
        Suggested Readings:
        <ol>
          <li><a href="http://www.deeplearningbook.org/contents/rnn.html">Sequence Modeling: Recurrent and Recursive Neural Nets</a> (Sections 10.3, 10.5, 10.7-10.12)</li>
          <li><a href="http://ai.dinfo.unifi.it/paolo//ps/tnn-94-gradient.pdf">Learning long-term dependencies with gradient descent is difficult</a> (one of the original vanishing gradient papers)</li>
          <li><a href="https://arxiv.org/pdf/1211.5063.pdf">On the difficulty of training Recurrent Neural Networks</a> (proof of vanishing gradient problem)</li>
          <li><a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/lectures/vanishing_grad_example.html">Vanishing Gradients Jupyter Notebook</a> (demo for feedforward networks)</li>
          <li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a> (blog post overview)</li>
          <!-- <li><a href="https://arxiv.org/pdf/1504.00941.pdf">A simple way to initialize recurrent networks of rectified linear units</a></li> -->
        </ol>
      </td>
      <td></td>
      <td></td>
    </tr>
  
    <tr>
      <td>Tue Jan 25</td>
      <td>Machine Translation, Attention, Subword Models
        <br>
        [<a href="slides/cs224n-2022-lecture07-nmt.pdf">slides</a>]
        [<a href="readings/cs224n-2019-notes06-NMT_seq2seq_attention.pdf">notes</a>]
      </td>
      <td>
        Suggested Readings:
        <ol>
          <li><a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1162/syllabus.shtml">Statistical Machine Translation slides, CS224n 2015</a> (lectures 2/3/4)</li>
          <li><a href="https://www.cambridge.org/core/books/statistical-machine-translation/94EADF9F680558E13BE759997553CDE5">Statistical Machine Translation</a> (book by Philipp Koehn)</li>
          <li><a href="https://www.aclweb.org/anthology/P02-1040.pdf">BLEU</a> (original paper)</li>
          <li><a href="https://arxiv.org/pdf/1409.3215.pdf">Sequence to Sequence Learning with Neural Networks</a> (original seq2seq NMT paper)</a></li>
          <li><a href="https://arxiv.org/pdf/1211.3711.pdf">Sequence Transduction with Recurrent Neural Networks</a> (early seq2seq speech recognition paper)</li>
          <li><a href="https://arxiv.org/pdf/1409.0473.pdf">Neural Machine Translation by Jointly Learning to Align and Translate</a> (original seq2seq+attention paper)</li>
          <li><a href="https://distill.pub/2016/augmented-rnns/">Attention and Augmented Recurrent Neural Networks</a> (blog post overview)</li>
          <li><a href="https://arxiv.org/pdf/1703.03906.pdf">Massive Exploration of Neural Machine Translation Architectures</a> (practical advice for hyperparameter choices)</li>
          <li><a href="https://arxiv.org/abs/1604.00788.pdf">Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models</a></li>
          <li><a href="https://arxiv.org/pdf/1808.09943.pdf">Revisiting Character-Based Neural Machine Translation with Capacity and Compression</a></li>
        </ol>
      </td>
      <td>Assignment 4 <b><font color="green">out</font></b>
          <br>
          [<a href="assignments/a4.zip">code</a>]<br>
          [<a href="assignments/a4.pdf">handout</a>]<br>
          [<a href="assignments/a4_latex.zip">latex template</a>]<br>
          [<a href="https://docs.google.com/document/d/10rhknu-xJJCHUQx3DPqKuHT35EqftmZ1rEdR1nJoBFo/edit?usp=sharing">Azure Guide</a>]<br>
          [<a href="https://docs.google.com/document/d/1jtANWXbIYXMZO_2X7jupauPxcEbz-TVJkdatg4gzOdk/edit?usp=sharing">Practical Guide to VMs</a>]<br>
      </td>
      <td>Assignment 3 <b><font color="red">due</font></b></td>
    </tr>
  
    <tr>
      <td>Thu Jan 27</td>
      <td>Final Projects: Custom and Default; Practical Tips
        <br>
        [<a href="slides/cs224n-2022-lecture08-final-project.pdf">slides</a>]
        [<a href="project/custom-final-project-tips.pdf">Custom project tips</a>]
        <!--[<a href="readings/final-project-practical-tips.pdf">notes</a>]-->
      </td>
      <td>
        Suggested Readings:
        <ol>
          <li><a href="https://www.deeplearningbook.org/contents/guidelines.html">Practical Methodology</a> (<i>Deep Learning</i> book chapter)</li>
        </ol>
      </td>
      <td>Project Proposal <b><font color="green">out</font></b>
          <br>
          [<a href="project/project-proposal-instructions-2022.pdf">instructions</a>]
          <br><br>
          Default Final Project <b><font color="green">out</font></b>
          <br>
          [<a href="project/default-final-project-handout-squad-track.pdf">handout (IID SQuAD track)</a>]
          <br>
          [<a href="project/default-final-project-handout-robustqa-track.pdf">handout (Robust QA track)</a>]
          <!--[<a href="https://github.com/minggg/squad">code</a>]-->
      </td>
      <td></td>
    </tr>
  
    <tr>
      <td>Tue Feb 1</td>
      <td>Transformers <i>(by Anna Goldie)</i>
      <br>
      [<a href="slides/cs224n-2022-lecture09-transformers.pdf">slides</a>]
      <!--[<a href="readings/cs224n-2019-notes07-QA.pdf">notes</a>]-->
      </td>
      <td>
        Suggested Readings:
        <ol>
          <li><a href="http://web.stanford.edu/class/cs224n/project/default-final-project-handout-squad-track.pdf">Project Handout (IID SQuAD track)</a>
          </li>
          <li><a href="http://web.stanford.edu/class/cs224n/project/default-final-project-handout-robustqa-track.pdf">Project Handout (Robust QA track)</a>
          </li>
          <li><a href="https://arxiv.org/abs/1706.03762.pdf">Attention Is All You Need</a>
          </li>
          <li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>
          </li>
          <li><a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Transformer (Google AI blog post)</a>
          </li>
          <li><a href="https://arxiv.org/pdf/1607.06450.pdf">Layer Normalization</a></li>
          <li><a href="https://arxiv.org/pdf/1802.05751.pdf">Image Transformer</a></li>
          <li><a href="https://arxiv.org/pdf/1809.04281.pdf">Music Transformer: Generating music with long-term structure</a></li>
        </ol>
      </td>
      <td></td>
      <td></td>
    </tr>
  
    <tr>
      <td>Thu Feb 3</td>
      <td>More about Transformers and Pretraining <i>(by Anna Goldie)</i>
        <br>
        [<a href="slides/cs224n-2022-lecture10-pretraining.pdf">slides</a>]
        <!--[<a href="readings/cs224n-2019-notes07-QA.pdf">notes</a>]-->
      </td>
      <td>
      Suggested Readings:
        <ol>
          <li>
            <a href="https://arxiv.org/pdf/1810.04805.pdf">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>
          </li>
          <li>
             <a href="https://arxiv.org/abs/1902.06006.pdf">Contextual Word Representations: A Contextual Introduction</a>
          </li>
          <li><a href="http://jalammar.github.io/illustrated-bert/">The Illustrated BERT, ELMo, and co.</a></li>
          <li><a href="https://web.stanford.edu/~jurafsky/slp3/11.pdf">Martin & Jurafsky Chapter on Transfer Learning</a></li>
        </ol>
      </td>
      <td>Assignment 5 <b><font color="green">out</font></b>
        <br>
        [<a href="assignments/a5.zip">code</a>]<br>
        [<a href="assignments/a5.pdf">handout</a>]<br>
        [<a href="assignments/a5_latex.zip">latex template</a>]
      </td>
      <td></td>
    </tr>
  
    <tr class="warning">
      <td>Fri Feb 4</td>
      <td>Hugging Face Transformers Tutorial Session
      </td>
      <td>
        <i class="fa fa-clock-o"></i> 1:30pm - 2:30pm<br>Thornton 102 (will be recorded)
      </td>
      <td><a href="https://colab.research.google.com/drive/1pxc-ehTtnVM72-NViET_D2ZqOlpOi2LH?usp=sharing">Colab</a></td>
      <td></td>
    </tr>
  
    <tr>
      <td>Tue Feb 8</td>
      <td>Question Answering
        <br>
        [<a href="slides/Danqi-QA-slides-2022.pdf">slides</a>]
        [<a href="readings/cs224n-2019-notes07-QA.pdf">notes</a>]
      </td>
      <td>
      Suggested Readings:
        <ol>
          <li>
            <a href="https://arxiv.org/pdf/1606.05250.pdf">SQuAD: 100,000+ Questions for Machine Comprehension of Text</a>
          </li>
          <li>
            <a href="https://arxiv.org/pdf/1611.01603.pdf">Bidirectional Attention Flow for Machine Comprehension</a>
          </li>
          <li>
            <a href="https://arxiv.org/pdf/1704.00051.pdf">Reading Wikipedia to Answer Open-Domain Questions</a>
          </li>
          <li>
            <a href="https://arxiv.org/pdf/1906.00300.pdf">Latent Retrieval for Weakly Supervised Open Domain Question Answering</a>
          </li>
          <li>
            <a href="https://arxiv.org/pdf/2004.04906.pdf">Dense Passage Retrieval for Open-Domain Question Answering</a>
          </li>
          <li>
            <a href="https://arxiv.org/pdf/2012.12624.pdf">Learning Dense Representations of Phrases at Scale</a>
          </li>
        </ol>
      </td>
      <td></td>
      <td>Project Proposal <b><font color="red">due</font></b><br>Assignment 4 <b><font color="red">due</font></b></td>
    </tr>
  
    <tr>
      <td>Thu Feb 10</td>
      <td>Natural Language Generation
        <br>
        [<a href="slides/cs224n-2022-lecture12-generation-final.pdf">slides</a>]
      </td>
      <td>
      Suggested readings:
        <ol>
          <li>
          <a href="https://arxiv.org/abs/1904.09751.pdf">The Curious Case of Neural Text Degeneration</a>
          </li>
          <li>
          <a href="https://arxiv.org/abs/1704.04368.pdf">Get To The Point: Summarization with Pointer-Generator Networks</a>
          </li>
          <li>
          <a href="https://arxiv.org/abs/1805.04833.pdf">Hierarchical Neural Story Generation</a>
          </li>
          <li>
          <a href="https://arxiv.org/abs/1603.08023.pdf">How NOT To Evaluate Your Dialogue System</a>
          </li>
        </ol>
      </td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Tue Feb 15</td>
        <td>Integrating knowledge in language models
          [<a href="slides/cs224n-2022-lecture-knowledge.pdf">slides</a>]
        </td>
      <td>
      Suggested readings:
      <ol>
        <li>
        <a href="https://arxiv.org/pdf/1905.07129.pdf">ERNIE: Enhanced Language Representation with Informative Entities</a>
        </li>
        <li>
        <a href="https://arxiv.org/pdf/1906.07241.pdf">Barackâ€™s Wife Hillary: Using Knowledge Graphs for Fact-Aware Language Modeling</a>
        </li>
        <li>
        <a href="https://arxiv.org/pdf/1912.09637.pdf">Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model</a>
        </li>
        <li>
        <a href="https://www.aclweb.org/anthology/D19-1250.pdf">Language Models as Knowledge Bases?</a>
        </li>
      </ol>
      </td>
      <td>Project Milestone <b><font color="green">out</font></b><br>[<a href="project/CS224N_Final_Project_Milestone_Instructions.pdf">Instructions</a>]</td>
      <td></td>
    </tr>
  
    <tr>
      <td>Thu Feb 17</td>
      <td>Bias, toxicity, and fairness<br>
        <i>(by <a href="https://homes.cs.washington.edu/~msap/">Maarten Sap</a>)</i>
      <!--<br>
      [<a href="slides/cs224n-2021-lecture14-t5.pdf">slides</a>]-->
      </td>
      <td>
      Suggested readings:
      <ol>
        <li><a href="https://homes.cs.washington.edu/~msap/pdfs/sap2019risk.pdf">The Risk of Racial Bias in Hate Speech Detection</a></li>
        <li><a href="https://homes.cs.washington.edu/~msap/social-bias-frames/">Social Bias Frames</a></li>
        <li><a href="https://arxiv.org/abs/2010.13816">PowerTransformer: Unsupervised Controllable Revision for Biased Language Correction</a></li>
      </ol>
      </td>
      <td></td>
      <td>Assignment 5 <b><font color="red">due</font></b></td>
    </tr>
  
    <tr>
      <td>Tue Feb 22</td>
      <td>Retrieval Augmented Models + Knowledge<br>
        <i>(by <a href="https://www.kelvinguu.com/">Kelvin Guu</a>)</i>
      <br>
      [<a href="slides/cs224n-2022-lecture15-guu.pdf">slides</a>]<br>
      <!--[<a href="https://docs.google.com/presentation/d/1LhL5FatjI19FaVyxKGC5Wdk2sFIQIbeKQcdJoYHebDo/edit?resourcekey=0-1_C4Nt7qCSghO1pP3AT5bg#slide=id.p">slides (GDrive link)</a>]-->
      </td>
      <td>
      Suggested readings:
      <ol>
        <li><a href="https://arxiv.org/abs/2202.05262">Locating and Editing Factual Knowledge in GPT</a></li>
        <li><a href="https://arxiv.org/abs/2201.08239">LaMDA: Language Models for Dialog Applications</a></li>
        <li><a href="https://arxiv.org/abs/2002.08909">REALM: Retrieval-Augmented Language Model Pre-Training</a></li>
      </ol>
      </td>
      <td></td>
      <td></td>
    </tr>
  
    <tr>
      <td>Thu Feb 24</td>
      <td>ConvNets, Tree Recursive Neural Networks and Constituency Parsing
      <br>
      [<a href="slides/cs224n-2022-lecture16-CNN-TreeRNN.pdf">slides</a>]
      </td>
      <td>Suggested readings:
        <ol>
          <li><a href="https://arxiv.org/abs/1408.5882.pdf">Convolutional Neural Networks for Sentence Classification</a></li>
          <li><a href="https://arxiv.org/abs/1207.0580">Improving neural networks by preventing co-adaptation of feature detectors</a></li>
          <li><a href="https://arxiv.org/pdf/1404.2188.pdf">A Convolutional Neural Network for Modelling Sentences</a></li>
          <li><a href="http://www.aclweb.org/anthology/P13-1045">Parsing with Compositional Vector Grammars.</a></li>
          <li><a href="https://arxiv.org/pdf/1805.01052.pdf">Constituency Parsing with a Self-Attentive Encoder</a></li>
        </ol>
      </td>
      <td></td>
      <td>Project Milestone <b><font color="red">due</font></b></td>
    </tr>
  
    <tr>
      <td>Tue Mar 1</td>
      <td>Scaling laws for large models<br>
        <i>(by <a href="https://sites.krieger.jhu.edu/jared-kaplan/">Jared Kaplan</a>)</i>
      </td>
      <td>Suggested readings:
        <ol>
          <li><a href="https://arxiv.org/abs/2001.08361">Scaling Laws for Neural Language Models</a></li>
        </ol>
      </td>
      <td></td>
      <td></td>
    </tr>
  
    <tr>
      <td>Thu Mar 3</td>
      <td>Coreference<br>
        [<a href="slides/cs224n-2022-lecture18-coref.pdf">slides</a>]
      </td>
      <td>Suggested readings:
      <ol>
        <li><a href="https://web.stanford.edu/~jurafsky/slp3/21.pdf">Coreference Resolution Chapter from Jurafsky and Martin</a></li>
        <li><a href="https://arxiv.org/pdf/1707.07045.pdf">End-to-end Neural Coreference Resolution</a></li>
      </ol>
      </td>
      <td></td>
      <td></td>
    </tr>
  
    <tr>
      <td>Tue Mar 8</td>
      <td>Editing Neural Networks<br>
        [<a href="slides/cs224n-2022-lecture-editing.pdf">slides</a>]
      </td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
  
    <tr>
      <td>Thu Mar 10</td>
      <td>[Lecture Cancelled]
        <!--<br>
        [<a href="slides/cs224n-2021-lecture18-future.pdf">slides</a>]-->
      </td>
      <td>Extra project office hours available during usual lecture time, see Ed.</td>
      <td></td>
      <td></td>
    </tr>
<!--
  stanford ends
-->

<!-- End of days section -->
  </div>
  <!--   Core JS Files   -->
  <script src="./assets/js/core/jquery.min.js" type="text/javascript"></script>
  <script src="./assets/js/core/popper.min.js" type="text/javascript"></script>
  <script src="./assets/js/core/bootstrap.min.js" type="text/javascript"></script>
  <script src="./assets/js/plugins/perfect-scrollbar.jquery.min.js"></script>
  <!--  Plugin for Switches, full documentation here: http://www.jque.re/plugins/version3/bootstrap.switch/ -->
  <script src="./assets/js/plugins/bootstrap-switch.js"></script>
  <!--  Plugin for the Sliders, full documentation here: http://refreshless.com/nouislider/ -->
  <script src="./assets/js/plugins/nouislider.min.js" type="text/javascript"></script>
  <script src="./assets/js/plugins/moment.min.js"></script>
  <script src="./assets/js/plugins/datetimepicker.js" type="text/javascript"></script>
  <script src="./assets/js/plugins/bootstrap-datepicker.min.js"></script>
  <!-- Control Center for Argon UI Kit: parallax effects, scripts for the example pages etc -->
  <!--  Google Maps Plugin    -->
  <script src="https://maps.googleapis.com/maps/api/js?key=YOUR_KEY_HERE"></script>
  <script src="./assets/js/argon-design-system.min.js?v=1.2.2" type="text/javascript"></script>
  <script>
    function scrollToDownload() {

      if ($('.section-download').length != 0) {
        $("html, body").animate({
          scrollTop: $('.section-download').offset().top
        }, 1000);
      }
    }
  </script>
  <script src="https://cdn.trackjs.com/agent/v3/latest/t.js"></script>
  <script>
    window.TrackJS &&
      TrackJS.install({
        token: "ee6fab19c5a04ac1a32a645abde4613a",
        application: "argon-design-system-pro"
      });
  </script>
</body>

</html>